--- svm_learn.c	2004-08-27 17:56:21.000000000 -0400
+++ svm_learn.c	2005-02-16 09:22:04.879654776 -0500
@@ -15,10 +15,14 @@
 /*   use of this software.                                             */
 /*                                                                     */
 /***********************************************************************/
+#include "svm_common.h"
+#include "svm_learn.h"
 
-
-# include "svm_common.h"
-# include "svm_learn.h"
+#ifdef MATLAB_MEX
+#include "mex.h"
+#include "global.h"
+#include "mexcommon.h"
+#endif
 
 
 /* interface to QP-solver */
@@ -65,8 +69,11 @@
   double heldout_c=0,r_delta_sq=0,r_delta,r_delta_avg;
   long *index,*index2dnum;
   double *weights;
+  double tmp;
   CFLOAT *aicache;  /* buffer to keep one row of hessian */
 
+
+
   double *xi_fullset; /* buffer for storing xi on full sample in loo */
   double *a_fullset;  /* buffer for storing alpha on full sample in loo */
   TIMING timing_profile;
@@ -105,6 +112,8 @@
   model->alpha = (double *)my_malloc(sizeof(double)*(totdoc+2));
   model->index = (long *)my_malloc(sizeof(long)*(totdoc+2));
 
+
+
   model->at_upper_bound=0;
   model->b=0;	       
   model->supvec[0]=0;  /* element 0 reserved and empty for now */
@@ -127,6 +136,13 @@
   r_delta_sq=r_delta*r_delta;
 
   r_delta_avg=estimate_r_delta_average(docs,totdoc,kernel_parm);
+
+  /* store some extra values in the model */
+#ifdef MATLAB_MEX
+  model->r_delta_sq = r_delta_sq;
+  model->r_delta_avg = r_delta_avg;
+#endif
+
   if(learn_parm->svm_c == 0.0) {  /* default value for C */
     learn_parm->svm_c=1.0/(r_delta_avg*r_delta_avg);
     if(verbosity>=1) 
@@ -207,10 +223,13 @@
     for(i=0;i<totdoc;i++) {    /* copy initial alphas */
       a[i]=alpha[i];
     }
-    free(index);
-    free(index2dnum);
-    free(weights);
-    free(aicache);
+
+    /** changed to use my_free */
+    my_free(index);
+    my_free(index2dnum);
+    my_free(weights);
+    my_free(aicache);
+
     if(verbosity>=1) {
       printf("done.\n");  fflush(stdout);
     }   
@@ -251,7 +270,13 @@
 				     &maxdiff,(long)-1,
 				     (long)1);
   
+
+  /* when using MATLAB, this must always be executed - but possibly
+   * not displayed .   It must be run to populate the model variable.
+   */
+#ifndef MATLAB_MEX
   if(verbosity>=1) {
+#endif
     if(verbosity==1) printf("done. (%ld iterations)\n",iterations);
 
     misclassified=0;
@@ -260,6 +285,7 @@
 	misclassified++;
     }
 
+    if (verbosity>=1) 
     printf("Optimization finished (%ld misclassified, maxdiff=%.5f).\n",
 	   misclassified,maxdiff); 
 
@@ -276,6 +302,7 @@
         (100.0*timing_profile.time_select)/(float)(runtime_end-runtime_start));
     }
     else {
+      if (verbosity>=1)
       printf("Runtime in cpu-seconds: %.2f\n",
 	     (runtime_end-runtime_start)/100.0);
     }
@@ -285,6 +312,7 @@
       for(i=0;i<totdoc;i++) 
 	if(inconsistent[i]) 
 	  inconsistentnum++;
+      if (verbosity>=1)
       printf("Number of SV: %ld (plus %ld inconsistent examples)\n",
 	     model->sv_num-1,inconsistentnum);
     }
@@ -296,11 +324,15 @@
 	    learn_parm->epsilon_a)) 
 	  upsupvecnum++;
       }
+      if (verbosity >=1)
       printf("Number of SV: %ld (including %ld at upper bound)\n",
 	     model->sv_num-1,upsupvecnum);
     }
     
+#ifndef MATLAB_MEX
     if((verbosity>=1) && (!learn_parm->skip_final_opt_check)) {
+#endif
+      if (!learn_parm->skip_final_opt_check) {  
       loss=0;
       model_length=0; 
       for(i=0;i<totdoc;i++) {
@@ -309,25 +341,41 @@
 	model_length+=a[i]*label[i]*lin[i];
       }
       model_length=sqrt(model_length);
+
+/* #ifdef MATLAB_MEX */
+/* 	/\* this isn't really an errorr,but will probably confuse MATLAB fns. *\/ */
+/* 	if (model_length == 0) */
+/* 	  mexErrMsgTxt(ERR006); */
+/* #endif */
+
+	example_length=estimate_sphere(model,kernel_parm); 
+	tmp =  estimate_margin_vcdim(model,model_length,example_length, kernel_parm);
+	
+#ifdef MATLAB_MEX
+	model->model_length = model_length;
+	model->loss = loss;
+	model->vcdim = tmp;
+	model->example_length = example_length;
+#endif
+	if (verbosity >=1) {
       fprintf(stdout,"L1 loss: loss=%.5f\n",loss);
       fprintf(stdout,"Norm of weight vector: |w|=%.5f\n",model_length);
-      example_length=estimate_sphere(model,kernel_parm); 
       fprintf(stdout,"Norm of longest example vector: |x|=%.5f\n",
 	      length_of_longest_document_vector(docs,totdoc,kernel_parm));
-      fprintf(stdout,"Estimated VCdim of classifier: VCdim<=%.5f\n",
-	      estimate_margin_vcdim(model,model_length,example_length,
-				    kernel_parm));
+	  fprintf(stdout,"Estimated VCdim of classifier: VCdim<=%.5f\n", tmp);
+	}
+	
       if((!learn_parm->remove_inconsistent) && (!transduction)) {
-	runtime_start_xa=get_runtime();
+	  
 	if(verbosity>=1) {
 	  printf("Computing XiAlpha-estimates..."); fflush(stdout);
 	}
+	  runtime_start_xa=get_runtime();
 	compute_xa_estimates(model,label,unlabeled,totdoc,docs,lin,a,
 			     kernel_parm,learn_parm,&(model->xa_error),
 			     &(model->xa_recall),&(model->xa_precision));
 	if(verbosity>=1) {
 	  printf("done\n");
-	}
 	printf("Runtime for XiAlpha-estimates in cpu-seconds: %.2f\n",
 	       (get_runtime()-runtime_start_xa)/100.0);
 	
@@ -338,6 +386,7 @@
 	fprintf(stdout,"XiAlpha-estimate of the precision: precision=>%.2f%% (rho=%.2f,depth=%ld)\n",
 		model->xa_precision,learn_parm->rho,learn_parm->xa_depth);
       }
+	}
       else if(!learn_parm->remove_inconsistent) {
 	estimate_transduction_quality(model,label,unlabeled,totdoc,docs,lin);
       }
@@ -345,8 +394,10 @@
     if(verbosity>=1) {
       printf("Number of kernel evaluations: %ld\n",kernel_cache_statistic);
     }
+#ifndef MATLAB_MEX      
   }
-
+  }
+#endif
 
   /* leave-one-out testing starts now */
   if(learn_parm->compute_loo) {
@@ -416,7 +467,6 @@
       }
     } /* end of leave-one-out loop */
 
-
     if(verbosity>=1) {
       printf("\nRetrain on full problem"); fflush(stdout); 
     }
@@ -435,35 +485,40 @@
     model->loo_precision=(trainpos-loo_count_pos)/
       (double)(trainpos-loo_count_pos+loo_count_neg)*100.0;
     if(verbosity >= 1) {
-      fprintf(stdout,"Leave-one-out estimate of the error: error=%.2f%%\n",
+      printf("Leave-one-out estimate of the error: error=%.2f%%\n",
 	      model->loo_error);
-      fprintf(stdout,"Leave-one-out estimate of the recall: recall=%.2f%%\n",
+      printf("Leave-one-out estimate of the recall: recall=%.2f%%\n",
 	      model->loo_recall);
-      fprintf(stdout,"Leave-one-out estimate of the precision: precision=%.2f%%\n",
+      printf("Leave-one-out estimate of the precision: precision=%.2f%%\n",
 	      model->loo_precision);
-      fprintf(stdout,"Actual leave-one-outs computed:  %ld (rho=%.2f)\n",
+      printf("Actual leave-one-outs computed:  %ld (rho=%.2f)\n",
 	      loocomputed,learn_parm->rho);
       printf("Runtime for leave-one-out in cpu-seconds: %.2f\n",
 	     (double)(get_runtime()-runtime_start_loo)/100.0);
-    }
-  }
+    } /* end if verbosity... */
+  } /* end if compute_loo */
     
+#ifndef MATLAB_MEX  
   if(learn_parm->alphafile[0])
     write_alphas(learn_parm->alphafile,a,label,totdoc);
+#else
+  model->a = (double *)malloc(sizeof(double) * totdoc);
+  for (i = 0; i < totdoc; i++)
+    model->a[i] = a[i] * label[i];
+#endif
   
   shrink_state_cleanup(&shrink_state);
-  free(label);
-  free(inconsistent);
-  free(unlabeled);
-  free(c);
-  free(a);
-  free(a_fullset);
-  free(xi_fullset);
-  free(lin);
-  free(learn_parm->svm_cost);
+  my_free(label);   /* change to use my_free */
+  my_free(inconsistent);
+  my_free(unlabeled);
+  my_free(c);
+  my_free(a);
+  my_free(a_fullset);
+  my_free(xi_fullset);
+  my_free(lin);
+  my_free(learn_parm->svm_cost);
 }
 
-
 /* Learns an SVM regression model based on the training data in
    docs/label. The resulting model is returned in the structure
    model. */
@@ -498,6 +553,11 @@
   DOC **docs_org;
   long *label;
 
+#ifdef MATLAB_MEX
+  /* TODO modify svm_learn_regression for MATLAB */
+  mexErrMsgTxt(ERR008);
+#endif
+
   /* set up regression problem in standard form */
   docs_org=docs;
   docs = (DOC **)my_malloc(sizeof(DOC)*2*totdoc);
@@ -514,6 +574,7 @@
   }
   totdoc*=2;
 
+
   /* need to get a bigger kernel cache */
   if(*kernel_cache) {
     kernel_cache_size=(*kernel_cache)->buffsize*sizeof(CFLOAT)/(1024*1024);
@@ -533,6 +594,7 @@
 
   learn_parm->totwords=totwords;
 
+
   /* make sure -n value is reasonable */
   if((learn_parm->svm_newvarsinqp < 2) 
      || (learn_parm->svm_newvarsinqp > learn_parm->svm_maxqpsize)) {
@@ -688,16 +750,16 @@
   shrink_state_cleanup(&shrink_state);
   for(i=0;i<totdoc;i++)
     free_example(docs[i],0);
-  free(docs);
-  free(label);
-  free(inconsistent);
-  free(unlabeled);
-  free(c);
-  free(a);
-  free(a_fullset);
-  free(xi_fullset);
-  free(lin);
-  free(learn_parm->svm_cost);
+  my_free(docs);
+  my_free(label);
+  my_free(inconsistent);
+  my_free(unlabeled);
+  my_free(c);
+  my_free(a);
+  my_free(a_fullset);
+  my_free(xi_fullset);
+  my_free(lin);
+  my_free(learn_parm->svm_cost);
 }
 
 void svm_learn_ranking(DOC **docs, double *rankvalue, long int totdoc, 
@@ -722,6 +784,10 @@
   MODEL *pairmodel;
   SVECTOR *flow,*fhigh;
 
+#ifdef MATLAB_MEX
+  mexErrMsgTxt(ERR009);
+#endif
+	
   totpair=0;
   for(i=0;i<totdoc;i++) {
     for(j=i+1;j<totdoc;j++) {
@@ -836,16 +902,16 @@
   model->xa_recall=-1;
   model->xa_precision=-1;
 
-  free(alpha);
-  free(greater);
-  free(lesser);
-  free(target);
+  my_free(alpha);
+  my_free(greater);
+  my_free(lesser);
+  my_free(target);
 
   /* If you would like to output the original model on pairs of
      document, replace the following lines with '(*model)=(*pairmodel);' */
   for(i=0;i<totpair;i++)
     free_example(docdiff[i],1);
-  free(docdiff);
+  my_free(docdiff);
   free_model(pairmodel,0);
 }
 
@@ -1008,10 +1074,10 @@
     for(i=0;i<totdoc;i++) {    /* copy initial alphas */
       a[i]=alpha[i];
     }
-    free(index);
-    free(index2dnum);
-    free(weights);
-    free(aicache);
+    my_free(index);
+    my_free(index2dnum);
+    my_free(weights);
+    my_free(aicache);
     if(verbosity>=1) {
       printf("done.\n");  fflush(stdout);
     }   
@@ -1115,10 +1181,10 @@
       if(alphaslack[i] > learn_parm->epsilon_a)
 	svsetnum++;
     }
-    free(index);
-    free(index2dnum);
-    free(slack);
-    free(alphaslack);
+    my_free(index);
+    my_free(index2dnum);
+    my_free(slack);
+    my_free(alphaslack);
   }
   
   if((verbosity>=1) && (!learn_parm->skip_final_opt_check)) {
@@ -1159,13 +1225,13 @@
     write_alphas(learn_parm->alphafile,a,label,totdoc);
   
   shrink_state_cleanup(&shrink_state);
-  free(label);
-  free(unlabeled);
-  free(inconsistent);
-  free(c);
-  free(a);
-  free(lin);
-  free(learn_parm->svm_cost);
+  my_free(label);
+  my_free(unlabeled);
+  my_free(inconsistent);
+  my_free(c);
+  my_free(a);
+  my_free(lin);
+  my_free(learn_parm->svm_cost);
 }
 
 
@@ -1396,14 +1462,17 @@
       cache_multiple_kernel_rows(kernel_cache,docs,working2dnum,
 				 choosenum,kernel_parm); 
     
+
     if(verbosity>=2) t2=get_runtime();
     if(retrain != 2) {
+
       optimize_svm(docs,label,unlabeled,inconsistent,0.0,chosen,active2dnum,
 		   model,totdoc,working2dnum,choosenum,a,lin,c,learn_parm,
 		   aicache,kernel_parm,&qp,&epsilon_crit_org);
     }
 
     if(verbosity>=2) t3=get_runtime();
+
     update_linear_component(docs,label,active2dnum,a,a_old,working2dnum,totdoc,
 			    totwords,kernel_parm,kernel_cache,lin,aicache,
 			    weights);
@@ -1427,6 +1496,8 @@
       a_old[i]=a[i];
     }
 
+
+
     if(retrain == 2) {  /* reset inconsistent unlabeled examples */
       for(i=0;(i<totdoc);i++) {
 	if(inconsistent[i] && unlabeled[i]) {
@@ -1441,6 +1512,8 @@
 			     inconsistent,active2dnum,last_suboptimal_at,
 			     iteration,kernel_parm);
 
+
+
     if(verbosity>=2) {
       t6=get_runtime();
       timing_profile->time_select+=t1-t0;
@@ -1587,23 +1660,23 @@
     }
   } /* end of loop */
 
-  free(chosen);
-  free(last_suboptimal_at);
-  free(key);
-  free(selcrit);
-  free(selexam);
-  free(a_old);
-  free(aicache);
-  free(working2dnum);
-  free(active2dnum);
-  free(qp.opt_ce);
-  free(qp.opt_ce0);
-  free(qp.opt_g);
-  free(qp.opt_g0);
-  free(qp.opt_xinit);
-  free(qp.opt_low);
-  free(qp.opt_up);
-  free(weights);
+  my_free(chosen);
+  my_free(last_suboptimal_at);
+  my_free(key);
+  my_free(selcrit);
+  my_free(selexam);
+  my_free(a_old);
+  my_free(aicache);
+  my_free(working2dnum);
+  my_free(active2dnum);
+  my_free(qp.opt_ce);
+  my_free(qp.opt_ce0);
+  my_free(qp.opt_g);
+  my_free(qp.opt_g0);
+  my_free(qp.opt_xinit);
+  my_free(qp.opt_low);
+  my_free(qp.opt_up);
+  my_free(weights);
 
   learn_parm->epsilon_crit=epsilon_crit_org; /* restore org */
   model->maxdiff=(*maxdiff);
@@ -2014,28 +2087,28 @@
   } /* end of loop */
 
 
-  free(alphaslack);
-  free(slack);
-  free(chosen);
-  free(unlabeled);
-  free(inconsistent);
-  free(ignore);
-  free(last_suboptimal_at);
-  free(key);
-  free(selcrit);
-  free(selexam);
-  free(a_old);
-  free(aicache);
-  free(working2dnum);
-  free(active2dnum);
-  free(qp.opt_ce);
-  free(qp.opt_ce0);
-  free(qp.opt_g);
-  free(qp.opt_g0);
-  free(qp.opt_xinit);
-  free(qp.opt_low);
-  free(qp.opt_up);
-  free(weights);
+  my_free(alphaslack);
+  my_free(slack);
+  my_free(chosen);
+  my_free(unlabeled);
+  my_free(inconsistent);
+  my_free(ignore);
+  my_free(last_suboptimal_at);
+  my_free(key);
+  my_free(selcrit);
+  my_free(selexam);
+  my_free(a_old);
+  my_free(aicache);
+  my_free(working2dnum);
+  my_free(active2dnum);
+  my_free(qp.opt_ce);
+  my_free(qp.opt_ce0);
+  my_free(qp.opt_g);
+  my_free(qp.opt_g0);
+  my_free(qp.opt_xinit);
+  my_free(qp.opt_low);
+  my_free(qp.opt_up);
+  my_free(weights);
 
   learn_parm->epsilon_crit=epsilon_crit_org; /* restore org */
   model->maxdiff=(*maxdiff);
@@ -2107,12 +2180,14 @@
     long i;
     double *a_v;
 
+
     compute_matrices_for_optimization(docs,label,unlabeled,
 				      exclude_from_eq_const,eq_target,chosen,
 				      active2dnum,working2dnum,model,a,lin,c,
 				      varnum,totdoc,learn_parm,aicache,
 				      kernel_parm,qp);
 
+
     if(verbosity>=3) {
       printf("Running optimizer..."); fflush(stdout);
     }
@@ -2150,6 +2225,7 @@
   register long ki,kj,i,j;
   register double kernel_temp;
 
+
   if(verbosity>=3) {
     fprintf(stdout,"Computing qp-matrices (type %ld kernel [degree %ld, rbf_gamma %f, coef_lin %f, coef_const %f])...",kernel_parm->kernel_type,kernel_parm->poly_degree,kernel_parm->rbf_gamma,kernel_parm->coef_lin,kernel_parm->coef_const); 
     fflush(stdout);
@@ -3076,7 +3152,7 @@
 				    long int *key, 
 				    long int *chosen, 
 				    long int iteration)
-/* Use the feasible direction approach to select the next
+     /* Use the feasible direction approach to select the next
    qp-subproblem (see section 'Selecting a good working set'). Chooses
    a feasible direction at (pseudo) random to help jump over numerical
    problem. */
@@ -3227,13 +3303,13 @@
 
 void shrink_state_cleanup(SHRINK_STATE *shrink_state)
 {
-  free(shrink_state->active);
-  free(shrink_state->inactive_since);
+  my_free(shrink_state->active);
+  my_free(shrink_state->inactive_since);
   if(shrink_state->deactnum > 0) 
-    free(shrink_state->a_history[shrink_state->deactnum-1]);
-  free(shrink_state->a_history);
-  free(shrink_state->last_a);
-  free(shrink_state->last_lin);
+    my_free(shrink_state->a_history[shrink_state->deactnum-1]);
+  my_free(shrink_state->a_history);
+  my_free(shrink_state->last_a);
+  my_free(shrink_state->last_lin);
 }
 
 long shrink_problem(DOC **docs,
@@ -3381,10 +3457,10 @@
 	}
       }
     }
-    free(changed);
-    free(changed2dnum);
-    free(inactive);
-    free(inactive2dnum);
+    my_free(changed);
+    my_free(changed2dnum);
+    my_free(inactive);
+    my_free(inactive2dnum);
   }
   (*maxdiff)=0;
   for(i=0;i<totdoc;i++) {
@@ -3422,7 +3498,7 @@
       (shrink_state->a_history[shrink_state->deactnum-1])[i]=a[i];
     }
     for(t=shrink_state->deactnum-2;(t>=0) && shrink_state->a_history[t];t--) {
-      free(shrink_state->a_history[t]);
+      my_free(shrink_state->a_history[t]);
       shrink_state->a_history[t]=0;
     }
   }
@@ -3565,7 +3641,7 @@
     kernel_cache->max_elems=totdoc;
   }
 
-  free(keep);
+  my_free(keep);
 
   if(verbosity>=2) {
     printf("done.\n"); fflush(stdout);
@@ -3635,14 +3711,14 @@
 
 void kernel_cache_cleanup(KERNEL_CACHE *kernel_cache)
 {
-  free(kernel_cache->index);
-  free(kernel_cache->occu);
-  free(kernel_cache->lru);
-  free(kernel_cache->invindex);
-  free(kernel_cache->active2totdoc);
-  free(kernel_cache->totdoc2active);
-  free(kernel_cache->buffer);
-  free(kernel_cache);
+  my_free(kernel_cache->index);
+  my_free(kernel_cache->occu);
+  my_free(kernel_cache->lru);
+  my_free(kernel_cache->invindex);
+  my_free(kernel_cache->active2totdoc);
+  my_free(kernel_cache->totdoc2active);
+  my_free(kernel_cache->buffer);
+  my_free(kernel_cache);
 }
 
 long kernel_cache_malloc(KERNEL_CACHE *kernel_cache)
@@ -3814,8 +3890,8 @@
   (*precision)=(((double)totposex-(double)looposerror)
     /((double)totposex-(double)looposerror+(double)loonegerror))*100.0;
 
-  free(sv);
-  free(sv2dnum);
+  my_free(sv);
+  my_free(sv2dnum);
 }
 
 
@@ -3900,8 +3976,8 @@
     }
   }    
 
-  free(cache);
-  free(trow);
+  my_free(cache);
+  my_free(trow);
 
   /*  printf("Distribute[%ld](%ld)=%f, ",docnum,best_depth,best); */
   return(best);
@@ -4145,3 +4221,4 @@
   }
 }
 
+    

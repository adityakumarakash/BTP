%% The lines that are not written within % .. % are not to be tampered with!!

\documentclass[twoside]{article}
\setlength{\oddsidemargin}{0.25 in}
\setlength{\evensidemargin}{-0.25 in}
\setlength{\topmargin}{-0.6 in}
\setlength{\textwidth}{6.5 in}
\setlength{\textheight}{8.5 in}
\setlength{\headsep}{0.75 in}
\setlength{\parindent}{0 in}
\setlength{\parskip}{0.1 in}

%
% ADD PACKAGES here:
%

\usepackage{amsmath,amsfonts,amssymb,graphicx}
\usepackage{hyperref}
\graphicspath{ {} }
\newcommand*\rfrac[2]{{}^{#1}\!/_{#2}}



\newcommand{\lecture}[4]{
   \pagestyle{myheadings}
   \thispagestyle{plain}
   \newpage
   \noindent
   \begin{center}
   \framebox{
      \vbox{\vspace{2mm}
       \vspace{2mm}
       \hbox to 6.28in { {\Large \hfill Capturing confusion of user between labels  \hfill} }
       \vspace{2mm}
       \hbox to 6.28in { {\it \hfill By: #2} }
      \vspace{2mm}}
   }
   \end{center}

   \vspace*{4mm}
}



\renewcommand{\cite}[1]{[#1]}
\def\beginrefs{\begin{list}%
        {[\arabic{equation}]}{\usecounter{equation}
         \setlength{\leftmargin}{2.0truecm}\setlength{\labelsep}{0.4truecm}%
         \setlength{\labelwidth}{1.6truecm}}}
\def\endrefs{\end{list}}
\def\bibentry#1{\item[\hbox{[#1]}]}

%Use this command for a figure; it puts a figure in wherever you want it.
%usage: \fig{NUMBER}{SPACE-IN-INCHES}{CAPTION}
%This is for your help while typing the notes
\newcommand{\fig}[3]{
			\vspace{#2}
			\begin{center}
			Figure \thelecnum.#1:~#3
			\end{center}
	}
% Use these for theorems, lemmas, proofs, etc. 
% See how they are used, below 


\newcommand\E{\mathbb{E}}

\begin{document}

\lecture{Ganesh Ramakrishnan}{Aditya Kumar Akash}%In place of scribe-name, write down your names or your group names



%% THIS ENDS THE EXAMPLES. DON'T DELETE THE FOLLOWING LINE:
\section{Situation in which user is confused}
We first look at situations where user $u_i$ could be said to be confused between two labels $l_1$ and $l_2$. \\
Intuitively $u_i$ should be confused, when majority of his labelling is inverse of what is established by consensus maximization.
So consider following matrix with values being the number of videos :


\begin{center}
  \begin{tabular}{ | c || c | c | c | c |} 
    \hline
    $Consensus \downarrow, u_i\rightarrow$ & $l_1$ & $l_2$ & $l_1, l_2$ & Neither\\
    \hline
    \hline
    $l_1$ & $a_1$ & $a_2$ & $a_3$ & $a_4$\\
    \hline
    $l_2$ & $b_1$ & $b_2$ & $b_3$ & $b_4$\\
    \hline
    $l_1, l_2$ & $c_1$ & $c_2$ & $c_3$ & $c_4$\\
    \hline
    Neither & $d_1$ & $d_2$ & $d_3$ & $d_4$\\
    \hline    
  \end{tabular}
\end{center}

The arrow represents from whom the label is obtained. Thus rows stand for labels obtained from consensus model, while column stand for labels given by user $u_i$.\\

Now we could say that a user $u_i$ is confused between $l_1$ and $l_2$ when he uses them interchangeably. Thus relevant elements from matrix are  :

\begin{center}
  \begin{tabular}{ | c | c | c |} 
    \hline
    Consensus & $u_i$ & Video count \\
    \hline
    \hline
    $l_1$ & $l_2$ & $a_2$\\
    \hline
    $l_1$ & $l_1, l_2$ & $a_3$\\
    \hline
    $l_2$ & $l_1$ & $b_1$\\
    \hline
    $l_2$ & $l_1,l_2$ & $b_3$\\
    \hline
  \end{tabular}
\end{center}
In these cases he uses the two labels in disagreement with the consensus prediction. \\\\

Let,\\ $s = \sum_i(a_i+b_i+c_i+d_i)$, i.e. total number of videos he watched.  \\
$p_o(l_i|l_j)$ = Observed probability of putting label $l_i$ given that consensus is on only $l_j$ \\\\
Thus, $p_o(l_2|l_1) = (a_2 + a_3)/s$, $p_o(l_1|l_2) = (b_1 + b_3)/s$. \\
Now we must remove from these the probability of seeing label $l_i$, given label $l_j$ is used, by the consensus model. An example which better explains this would be - Suppose we labelled something as $Computer$, then given that, what would be the probability of assigning label $Automata$ or $Machine$ or $System$ or $Junk$ to that thing. Thus it is akin to keeping a label bias, and then looking at label probability distribution.  

\section{Label Biased Probability distribution}
Consider the MLCM-r model from the \textbf{Multilabel Consensus Classification, ICDM} paper. We get a label probability distribution with each group node, which stood for seeing a label $l_i$ when being in a group node of label $l_j$. Let us call this $p(reach\ l_i\ |\ start\ g_{l_j}^k)$, where $g^k_{l_j}$ stands for group node corresponding to label $l_j$ of $k^{th}$ labeller and $(reach\ l_i)$ stands for reaching any node group node with label $l_i$. It stands for probability of reaching any group node with label $l_i$ from a given group node of label $l_j$, in the random walk amongst the graph of group nodes. \\\\
Thus probability of reaching any node with label $l_i$, given we are starting from any node with label $l_j$, 
\begin{center}
$p(reach\ l_i\ |\ start\ l_j)\ =\  \sum_k\ p(reach\ l_i\ |\ start\ g_{l_j}^k)\ *\ p(start\ g_{l_j}\ |\ start\ l_j)$ 
\end{center}
Assuming that we uniformly select a group of given label to start with and there are $m$ number of labellers, $p(start\ g_{l_j}\ |\ start\ l_j) = \frac{1}{m}$. Hence,

\begin{center}
$p(reach\ l_i\ |\ start\ l_j)\ =\  \frac{1}{m}\ *\ \sum_k\ p(reach\ l_i\ |\ start\ g_{l_j}^k)$
\end{center}
For us $p(reach\ l_i\ |\ start\ l_j)$ would mean probability that consensus model has of seeing/putting label $l_i$, given label $l_j$ is used. This was what we wanted. Let us call this $p_e(l_i|l_j)$.

\section{Confusion}
Now based on the above sections, we would calculate confusion of a user between two labels.\\
Using similar concept as \href{https://en.wikipedia.org/wiki/Cohen's\_kappa}{$Cohen\ Kappa$} confusion could be given by :
\begin{center}
  $Confusion\ =\ I_1*\frac{p_o(l_1|l_2)\ -\ p_e(l_1|l_2)}{1\ -\ p_e(l_2|l_1)} + I_2*\frac{p_o(l_2|l_1)\ -\ p_e(l_2|l_1)}{1\ -\ p_e(l_2|l_1)}$
\end{center}
where $I_1,\ I_2$ are indicator variables to generate positive value of confusion, i.e. $I_1 = 1,\ if\ p_o(l_1|l_2)\ \geq\ p_e(l_1|l_2)$ else $0$, similar for $I_2$.\\\\
This measure of confusion takes into account the probability of disagreement due to model itself, and hence is expected to be more robust.
\end{document}
